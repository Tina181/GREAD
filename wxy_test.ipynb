{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/gread/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 实现kernel中每个位置为两个节点特征之差\n",
    "实现kernel中的$e_{i,j}= x_i - x_j$, $E=(e_{ij})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3]) \n",
      " torch.Size([8, 8, 3])\n",
      "tensor([0., 0., 0.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "n=8\n",
    "d=3\n",
    "x = torch.eye(n,d)\n",
    "x_expanded = x.unsqueeze(0).repeat(n, 1, 1)\n",
    "print(x.shape, '\\n', x_expanded.shape)\n",
    "diff = x_expanded - x.unsqueeze(1)  # (n,n,d)\n",
    "print(diff[2,2])    # 这里可见diff[m,m]均为0， diff[i,j]表示特征x_i与特征x_j的差值\n",
    "sq_dist = torch.sum(diff ** 2, dim=-1)  # (n,n) 计算特征之间的距离\n",
    "print(sq_dist[2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8746])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/gread/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1记录一下 log核和gauss核之前的写法(no sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   def hadamard_product_sparse(self, K, x):\n",
    "#     # 求解A⊙K(X(t))X(t)\n",
    "#       num_nodes = K.size(0)  # 节点数量\n",
    "#       edge_weight = torch.ones(self.edge_index.size(1), device=self.device) # edge_index对应的非零元的值\n",
    "#       A = torch.sparse_coo_tensor(self.edge_index, edge_weight, (num_nodes, num_nodes)).to(self.device) # torch.Size([2485, 2485])\n",
    "#       # 获取稀疏矩阵 A 的索引和对应的值\n",
    "#       indices = A.coalesce().indices()  # 合并重复的索引，并获取稀疏矩阵 A 的索引, torch.Size([2, 10138])\n",
    "#       values = A.coalesce().values()  # 获取稀疏矩阵 A 的值 torch.Size([10138])\n",
    "#       # 计算 A 和 K 的 Hadamard 乘积\n",
    "#       # 只计算 A 中非零元素对应位置的 K 的值\n",
    "#       hadamard_values = values * K[indices[0], indices[1]]  # torch.Size([10138])\n",
    "#       # 构建稀疏矩阵的结果\n",
    "#       AKX = torch_sparse.spmm(self.edge_index, hadamard_values, num_nodes, num_nodes, x)  # 稀疏矩阵和密集矩阵的乘积A⊙K(X(t))X(t) \n",
    "#       return AKX  # torch.Size([2485, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  '''def calculate_log_kernel(self, x):\n",
    "        n = x.size(0)  # n: 2485\n",
    "        kernel = torch.zeros((n, n))  # kernel.shape: torch.Size([2485, 2485])\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    diff = x[i] - x[j]  # x[i]:torch.Size([64])\n",
    "                    abs_diff = torch.abs(diff)   # abs_diff.shape: torch.Size([64])\n",
    "                    kernel[i, j] = torch.log(abs_diff.sum())  # 1 范数\n",
    "        return kernel'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_log_kernel(self, x):\n",
    "  #       n = x.size(0)  # 获取样本数量\n",
    "  #       x_expanded = x.unsqueeze(0).repeat(n, 1, 1)  # 扩展维度并复制，形状为 (n, n, d)\n",
    "  #       diff = x_expanded - x.unsqueeze(1)  # 计算每对样本之间的差，形状为 (n, n, d)\n",
    "  #       abs_diff = torch.abs(diff)  # 取绝对值\n",
    "  #       sum_abs_diff = abs_diff.sum(dim=-1)  # 对最后一个维度求和，得到每对样本的 1 范数差，形状为 (n, n)\n",
    "  #       s = 1e-4*torch.eye(n).to(x.device)\n",
    "  #       sum_abs_diff += s  # 加入对角线元素，防止分母为零\n",
    "\n",
    "  #       # 对角线为零（因为对角线上的元素表示样本自身与自身的差）\n",
    "  #       log_kernel = torch.zeros_like(sum_abs_diff)  # 创建一个形状相同的零矩阵\n",
    "  #       nonzero_indices = sum_abs_diff != 0  # 找到非零元素的位置\n",
    "  #       log_kernel[nonzero_indices] = torch.log(sum_abs_diff[nonzero_indices])  # 对非零元素取对数\n",
    "  #       return log_kernel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # def calculate_gauss_kernel(self, x):\n",
    "  #     n = x.size(0)  # 节点数量\n",
    "  #     d = x.size(1)  # 特征维度\n",
    "\n",
    "  #     # 扩展并计算所有节点对之间的差值\n",
    "  #     x_expanded = x.unsqueeze(0).repeat(n, 1, 1)  # 形状 (n, n, d)\n",
    "  #     diff = x_expanded - x.unsqueeze(1)  # 形状 (n, n, d)\n",
    "        \n",
    "  #     # 计算平方距离矩阵\n",
    "  #     sq_dist = torch.sum(diff ** 2, dim=-1)  # 形状 (n, n)\n",
    "      \n",
    "  #     # 使用向量化计算高斯核矩阵\n",
    "  #     factor = 1 / ((4 * math.pi * self.epsilon ** 2) ** (d / 2))\n",
    "  #     exponent = torch.exp(-sq_dist / (4 * self.epsilon ** 2))\n",
    "  #     kernel = factor * exponent\n",
    "  #     return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elif self.opt['reaction_term'] =='aggdiff-gauss':\n",
    "    #   # (A - I)X(t) ⊙ K(X(t))X(t)\n",
    "    #   K = self.calculate_gauss_kernel(x)\n",
    "    #   AKX = self.hadamard_product_sparse(K, x)  # torch.Size([2485, 64])\n",
    "    #   reaction = (ax-x)*(AKX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gread",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
